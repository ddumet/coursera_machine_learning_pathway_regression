{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Week 5: Feature Selection and LASSO (Interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will use LASSO to select features, building on a pre-implemented solver for LASSO. You will:\n",
    "* Run LASSO with different L1 penalties.\n",
    "* Choose best L1 penalty using a validation set.\n",
    "* Choose best L1 penalty using a validation set, with additional constraint on the size of subset.\n",
    "\n",
    "In the second notebook, you will implement your own LASSO solver, using coordinate descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard libs\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Machine Learning\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in house sales data\n",
    "\n",
    "Dataset is from house sales in King County, the region where the city of Seattle, WA is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_dtype={'bathrooms':float, 'waterfront':int, 'sqft_above':int,\n",
    "             'sqft_living15':float, 'grade':int, 'yr_renovated':int,\n",
    "             'price':float, 'bedrooms':float, 'zipcode':str, 'long':float,\n",
    "             'sqft_lot15':float, 'sqft_living':float, 'floors':str, 'condition':int,\n",
    "             'lat':float, 'date':str, 'sqft_basement':int, 'yr_built':int, 'id':str,\n",
    "             'sqft_lot':int, 'view':int}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>5650</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>5650.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>7242</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>7639.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720.0</td>\n",
       "      <td>8062.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>8080</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>7503.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0       3.0       1.00       1180.0   \n",
       "1  6414100192  20141209T000000  538000.0       3.0       2.25       2570.0   \n",
       "2  5631500400  20150225T000000  180000.0       2.0       1.00        770.0   \n",
       "3  2487200875  20141209T000000  604000.0       4.0       3.00       1960.0   \n",
       "4  1954400510  20150218T000000  510000.0       3.0       2.00       1680.0   \n",
       "\n",
       "   sqft_lot floors  waterfront  view     ...      grade  sqft_above  \\\n",
       "0      5650      1           0     0     ...          7        1180   \n",
       "1      7242      2           0     0     ...          7        2170   \n",
       "2     10000      1           0     0     ...          6         770   \n",
       "3      5000      1           0     0     ...          7        1050   \n",
       "4      8080      1           0     0     ...          8        1680   \n",
       "\n",
       "   sqft_basement  yr_built  yr_renovated  zipcode      lat     long  \\\n",
       "0              0      1955             0    98178  47.5112 -122.257   \n",
       "1            400      1951          1991    98125  47.7210 -122.319   \n",
       "2              0      1933             0    98028  47.7379 -122.233   \n",
       "3            910      1965             0    98136  47.5208 -122.393   \n",
       "4              0      1987             0    98074  47.6168 -122.045   \n",
       "\n",
       "   sqft_living15  sqft_lot15  \n",
       "0         1340.0      5650.0  \n",
       "1         1690.0      7639.0  \n",
       "2         2720.0      8062.0  \n",
       "3         1360.0      5000.0  \n",
       "4         1800.0      7503.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales = pd.read_csv(\"./data/kc_house_data.csv\", dtype=sales_dtype)\n",
    "sales.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in Week 2, we consider features that are some transformations of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales[\"sqft_living_sqrt\"] = np.sqrt(sales[\"sqft_living\"])\n",
    "sales[\"sqft_lot_sqrt\"] = np.sqrt(sales[\"sqft_lot\"])\n",
    "sales[\"bedrooms_square\"] = np.square(sales[\"bedrooms\"])\n",
    "\n",
    "# In the dataset, 'floors' was defined with type string, \n",
    "# so we'll convert them to float, before creating a new feature.\n",
    "sales[\"floors\"] = sales[\"floors\"].astype(float) \n",
    "sales[\"floors_square\"] = np.square(sales[\"floors\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Squaring bedrooms will increase the separation between not many bedrooms (e.g. 1) and lots of bedrooms (e.g. 4) since 1^2 = 1 but 4^2 = 16. Consequently this variable will mostly affect houses with many bedrooms.\n",
    "* On the other hand, taking square root of sqft_living will decrease the separation between big house and small house. The owner may not be exactly twice as happy for getting a house that is twice as big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn regression weights with L1 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us fit a model with all the features available, plus the features we just created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_features = [\"bedrooms\", \"bedrooms_square\", \"bathrooms\",\n",
    "                \"sqft_living\", \"sqft_living_sqrt\", \"sqft_lot\",\n",
    "                \"sqft_lot_sqrt\", \"floors\", \"floors_square\",\n",
    "                \"waterfront\", \"view\", \"condition\", \"grade\", \"sqft_above\",\n",
    "                \"sqft_basement\", \"yr_built\", \"yr_renovated\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the entire house dataset, learn Lasso regression weights using an L1 penalty of 5e2. Make sure to add \"normalize=True\" when creating the Lasso model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=500.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=True, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_all = Lasso(alpha=5e2, normalize=True)\n",
    "model_all.fit(sales[all_features], sales[\"price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find what features had non-zero weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nonzeros features: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of nonzeros features:\",np.count_nonzero(np.append(model_all.intercept_,model_all.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Showing weight for all features **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         weight\n",
      "intercept        -218136.214035\n",
      "bedrooms               0.000000\n",
      "bedrooms_square        0.000000\n",
      "bathrooms              0.000000\n",
      "sqft_living          134.439314\n",
      "sqft_living_sqrt       0.000000\n",
      "sqft_lot               0.000000\n",
      "sqft_lot_sqrt          0.000000\n",
      "floors                 0.000000\n",
      "floors_square          0.000000\n",
      "waterfront             0.000000\n",
      "view               24750.004586\n",
      "condition              0.000000\n",
      "grade              61749.103091\n",
      "sqft_above             0.000000\n",
      "sqft_basement          0.000000\n",
      "yr_built              -0.000000\n",
      "yr_renovated           0.000000\n"
     ]
    }
   ],
   "source": [
    "weight_index = all_features.copy()\n",
    "weight_index.insert(0,\"intercept\")\n",
    "features_weight = pd.DataFrame(np.append(model_all.intercept_,model_all.coef_), columns=[\"weight\"], index=weight_index)\n",
    "print(features_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a majority of the weights have been set to zero. So by setting an L1 penalty that's large enough, we are performing a subset selection. \n",
    "\n",
    "***QUIZ QUESTION***:\n",
    "According to this list of weights, which of the features have been chosen? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    weight\n",
      "intercept   -218136.214035\n",
      "sqft_living     134.439314\n",
      "view          24750.004586\n",
      "grade         61749.103091\n"
     ]
    }
   ],
   "source": [
    "nonzero_features = features_weight[features_weight[\"weight\"] != 0]\n",
    "print(nonzero_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting an L1 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find a good L1 penalty, we will explore multiple values using a validation set. Let us do three way split into train, validation, and test sets:\n",
    "* Split our sales data into 2 sets: training and test\n",
    "* Further split our training data into two sets: train, validation\n",
    "\n",
    "Be *very* careful that you use seed = 1 to ensure you get the same answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset from files provided for the class as not using GraphLab.\n",
    "\n",
    "Let's redefine our **sales_dtype** so that [floors] is read as a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_dtype={'bathrooms':float, 'waterfront':int, 'sqft_above':int,\n",
    "             'sqft_living15':float, 'grade':int, 'yr_renovated':int,\n",
    "             'price':float, 'bedrooms':float, 'zipcode':str, 'long':float,\n",
    "             'sqft_lot15':float, 'sqft_living':float, 'floors':float, 'condition':int,\n",
    "             'lat':float, 'date':str, 'sqft_basement':int, 'yr_built':int, 'id':str,\n",
    "             'sqft_lot':int, 'view':int}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_train = pd.read_csv(\"./data/wk3_kc_house_train_data.csv\", dtype=sales_dtype)\n",
    "sales_valid = pd.read_csv(\"./data/wk3_kc_house_valid_data.csv\", dtype=sales_dtype)\n",
    "sales_test = pd.read_csv(\"./data/wk3_kc_house_test_data.csv\", dtype=sales_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding 4 new features as previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training set\n",
    "sales_train[\"sqft_living_sqrt\"] = np.sqrt(sales_train[\"sqft_living\"])\n",
    "sales_train[\"sqft_lot_sqrt\"] = np.sqrt(sales_train[\"sqft_lot\"])\n",
    "sales_train[\"bedrooms_square\"] = np.square(sales_train[\"bedrooms\"])\n",
    "sales_train[\"floors_square\"] = np.square(sales_train[\"floors\"])\n",
    "\n",
    "# Validation set\n",
    "sales_valid[\"sqft_living_sqrt\"] = np.sqrt(sales_valid[\"sqft_living\"])\n",
    "sales_valid[\"sqft_lot_sqrt\"] = np.sqrt(sales_valid[\"sqft_lot\"])\n",
    "sales_valid[\"bedrooms_square\"] = np.square(sales_valid[\"bedrooms\"])\n",
    "sales_valid[\"floors_square\"] = np.square(sales_valid[\"floors\"])\n",
    "\n",
    "# Test set\n",
    "sales_test[\"sqft_living_sqrt\"] = np.sqrt(sales_test[\"sqft_living\"])\n",
    "sales_test[\"sqft_lot_sqrt\"] = np.sqrt(sales_test[\"sqft_lot\"])\n",
    "sales_test[\"bedrooms_square\"] = np.square(sales_test[\"bedrooms\"])\n",
    "sales_test[\"floors_square\"] = np.square(sales_test[\"floors\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write a loop that does the following:\n",
    "* For `l1_penalty` in [10^1, 10^1.5, 10^2, 10^2.5, ..., 10^7] (to get this in Python, type `np.logspace(1, 7, num=13)`.)\n",
    "    * Fit a regression model with a given `l1_penalty` on TRAIN data. Make sure to specify\n",
    "normalize=True in the constructor\n",
    "    * Compute the RSS on VALIDATION data (here you will want to use `.predict()`) for that `l1_penalty`\n",
    "* Report which `l1_penalty` produced the lowest RSS on validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an empty frame to hold the RSS for each value of l1_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_range = np.logspace(1, 7, num=13)\n",
    "rss_valid_df = pd.DataFrame(np.zeros((log_range.shape[0],1)), index=log_range, columns=[\"RSS_valid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for l1_penalty in log_range:\n",
    "    model = Lasso(alpha=l1_penalty, normalize=True)\n",
    "    model.fit(sales_train[all_features], sales_train[\"price\"])\n",
    "    rss_valid_df.loc[l1_penalty] = (np.square(model.predict(sales_valid[all_features]) - sales_valid[\"price\"]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RSS_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.000000e+01</th>\n",
       "      <td>3.982133e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.162278e+01</th>\n",
       "      <td>3.990419e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000000e+02</th>\n",
       "      <td>4.297916e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.162278e+02</th>\n",
       "      <td>4.637398e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000000e+03</th>\n",
       "      <td>6.458987e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.162278e+03</th>\n",
       "      <td>1.222507e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000000e+04</th>\n",
       "      <td>1.222507e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.162278e+04</th>\n",
       "      <td>1.222507e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000000e+05</th>\n",
       "      <td>1.222507e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.162278e+05</th>\n",
       "      <td>1.222507e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000000e+06</th>\n",
       "      <td>1.222507e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.162278e+06</th>\n",
       "      <td>1.222507e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000000e+07</th>\n",
       "      <td>1.222507e+15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 RSS_valid\n",
       "1.000000e+01  3.982133e+14\n",
       "3.162278e+01  3.990419e+14\n",
       "1.000000e+02  4.297916e+14\n",
       "3.162278e+02  4.637398e+14\n",
       "1.000000e+03  6.458987e+14\n",
       "3.162278e+03  1.222507e+15\n",
       "1.000000e+04  1.222507e+15\n",
       "3.162278e+04  1.222507e+15\n",
       "1.000000e+05  1.222507e+15\n",
       "3.162278e+05  1.222507e+15\n",
       "1.000000e+06  1.222507e+15\n",
       "3.162278e+06  1.222507e+15\n",
       "1.000000e+07  1.222507e+15"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rss_valid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** QUIZ QUESTIONS ***\n",
    "* **What was the best value for the `l1_penalty`?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS min value is: 3.982133273e+14 for l1_penalty: 10.0\n"
     ]
    }
   ],
   "source": [
    "print(\"RSS min value is:\", rss_valid_df[\"RSS_valid\"].min(),\"for l1_penalty:\",rss_valid_df[\"RSS_valid\"].idxmin())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **What is the RSS on TEST data of the model with the best `l1_penalty`?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-fit the model with l1_penalty = **`rss_valid_df[\"RSS_Valid\"].idxmin()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rss_test: 98467402552698.88\n"
     ]
    }
   ],
   "source": [
    "model = Lasso(alpha=rss_valid_df[\"RSS_valid\"].idxmin(), normalize=True)\n",
    "model.fit(sales_train[all_features], sales_train[\"price\"])\n",
    "rss_test = np.square(model.predict(sales_test[all_features]) - sales_test[\"price\"]).sum()\n",
    "print(\"rss_test:\", rss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUIZ QUESTION***\n",
    "* **Also, using this value of L1 penalty, how many nonzero weights do you have (this is including the intercept!)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nonzeros features: 15\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of nonzeros features:\",np.count_nonzero(np.append(model.intercept_,model.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        weight\n",
      "intercept         6.630156e+06\n",
      "bedrooms         -1.614456e+04\n",
      "bedrooms_square   3.732454e+02\n",
      "bathrooms         5.084124e+04\n",
      "sqft_living       6.178536e+02\n",
      "sqft_living_sqrt -4.441135e+04\n",
      "sqft_lot          7.856231e-01\n",
      "sqft_lot_sqrt    -7.011948e+02\n",
      "floors           -0.000000e+00\n",
      "floors_square     5.014200e+03\n",
      "waterfront        6.194888e+05\n",
      "view              3.804186e+04\n",
      "condition         2.499877e+04\n",
      "grade             1.287162e+05\n",
      "sqft_above        0.000000e+00\n",
      "sqft_basement     0.000000e+00\n",
      "yr_built         -3.293831e+03\n",
      "yr_renovated      1.005732e+01\n"
     ]
    }
   ],
   "source": [
    "weight_index = all_features.copy()\n",
    "weight_index.insert(0,\"intercept\")\n",
    "features_weight = pd.DataFrame(np.append(model.intercept_,model.coef_), columns=[\"weight\"], index=weight_index)\n",
    "print(features_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit the number of nonzero weights\n",
    "\n",
    "What if we absolutely wanted to limit ourselves to, say, 7 features? This may be important if we want to derive \"a rule of thumb\" --- an interpretable model that has only a few features in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to implement a simple, two phase procedure to achive this goal:\n",
    "1. Explore a large range of `l1_penalty` values to find a narrow region of `l1_penalty` values where models are likely to have the desired number of non-zero weights.\n",
    "2. Further explore the narrow region you found to find a good value for `l1_penalty` that achieves the desired sparsity.  Here, we will again use a validation set to choose the best value for `l1_penalty`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_nonzeros = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the larger range of values to find a narrow range with the desired sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement a loop that search through this space of possible `l1_penalty` values:\n",
    "\n",
    "* For `l1_penalty` in `np.logspace(1, 4, num=20)`:\n",
    "    * Fit a regression model with a given `l1_penalty` on TRAIN data.\n",
    "    * Extract the weights of the model and count the number of nonzeros. Save the number of nonzeros to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_range = np.logspace(1, 4, num=20)\n",
    "nonzeros_df = pd.DataFrame(np.zeros((log_range.shape[0],1)), index=log_range, columns=[\"non_zeros\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1_penalty_max = 0\n",
    "for l1_penalty in log_range:\n",
    "    model = Lasso(alpha=l1_penalty, normalize=True)\n",
    "    model.fit(sales_train[all_features], sales_train[\"price\"])\n",
    "    nb_nonzeros = np.count_nonzero(np.append(model.intercept_,model.coef_))\n",
    "    nonzeros_df.loc[l1_penalty] = nb_nonzeros\n",
    "    if nb_nonzeros > max_nonzeros:\n",
    "        l1_penalty_min = l1_penalty\n",
    "    if l1_penalty_max == 0 and nb_nonzeros < max_nonzeros:\n",
    "        l1_penalty_max = l1_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              non_zeros\n",
      "10.000000          15.0\n",
      "14.384499          15.0\n",
      "20.691381          15.0\n",
      "29.763514          15.0\n",
      "42.813324          13.0\n",
      "61.584821          12.0\n",
      "88.586679          11.0\n",
      "127.427499         10.0\n",
      "183.298071          7.0\n",
      "263.665090          6.0\n",
      "379.269019          6.0\n",
      "545.559478          6.0\n",
      "784.759970          5.0\n",
      "1128.837892         3.0\n",
      "1623.776739         3.0\n",
      "2335.721469         2.0\n",
      "3359.818286         1.0\n",
      "4832.930239         1.0\n",
      "6951.927962         1.0\n",
      "10000.000000        1.0\n"
     ]
    }
   ],
   "source": [
    "print(nonzeros_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of this large range, we want to find the two ends of our desired narrow range of `l1_penalty`.  At one end, we will have `l1_penalty` values that have too few non-zeros, and at the other end, we will have an `l1_penalty` that has too many non-zeros.  \n",
    "\n",
    "More formally, find:\n",
    "* The largest `l1_penalty` that has more non-zeros than `max_nonzero` (if we pick a penalty smaller than this value, we will definitely have too many non-zero weights)\n",
    "    * Store this value in the variable `l1_penalty_min` (we will use it later)\n",
    "* The smallest `l1_penalty` that has fewer non-zeros than `max_nonzero` (if we pick a penalty larger than this value, we will definitely have too few non-zero weights)\n",
    "    * Store this value in the variable `l1_penalty_max` (we will use it later)\n",
    "\n",
    "\n",
    "*Hint: there are many ways to do this, e.g.:*\n",
    "* Programmatically within the loop above\n",
    "* Creating a list with the number of non-zeros for each value of `l1_penalty` and inspecting it to find the appropriate boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUIZ QUESTIONS***\n",
    "\n",
    "What values did you find for `l1_penalty_min` and `l1_penalty_max`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1_penalty_min: 127.0\n",
      "l1_penalty_max: 264.0\n"
     ]
    }
   ],
   "source": [
    "print(\"l1_penalty_min:\",round(l1_penalty_min,0))\n",
    "print(\"l1_penalty_max:\",round(l1_penalty_max,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the narrow range of values to find the solution with the right number of non-zeros that has lowest RSS on the validation set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For `l1_penalty` in `np.linspace(l1_penalty_min,l1_penalty_max,20)`:\n",
    "    * Fit a regression model with a given `l1_penalty` on TRAIN data.\n",
    "    * Measure the RSS of the learned model on the VALIDATION set\n",
    "\n",
    "Find the model that the lowest RSS on the VALIDATION set and has sparsity *equal* to `max_nonzero`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lin_range = np.linspace(l1_penalty_min,l1_penalty_max,20)\n",
    "rss_valid_nonzeros_df = pd.DataFrame(np.zeros((lin_range.shape[0],2)), index=lin_range, columns=[\"RSS_valid\",\"non_zeros\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for l1_penalty in lin_range:\n",
    "    model = Lasso(alpha=l1_penalty, normalize=True)\n",
    "    model.fit(sales_train[all_features], sales_train[\"price\"])\n",
    "    rss_valid_nonzeros_df.loc[l1_penalty][\"RSS_valid\"] = (np.square(model.predict(sales_valid[all_features])\n",
    "                                                                    - sales_valid[\"price\"]).sum())\n",
    "    rss_valid_nonzeros_df.loc[l1_penalty][\"non_zeros\"] = np.count_nonzero(np.append(model.intercept_,model.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               RSS_valid  non_zeros\n",
      "127.427499  4.353747e+14       10.0\n",
      "134.597898  4.370092e+14       10.0\n",
      "141.768298  4.382361e+14        8.0\n",
      "148.938697  4.391589e+14        8.0\n",
      "156.109097  4.400374e+14        7.0\n",
      "163.279496  4.407775e+14        7.0\n",
      "170.449896  4.415667e+14        7.0\n",
      "177.620295  4.424064e+14        7.0\n",
      "184.790695  4.432967e+14        7.0\n",
      "191.961094  4.442398e+14        7.0\n",
      "199.131494  4.452307e+14        7.0\n",
      "206.301894  4.462689e+14        6.0\n",
      "213.472293  4.471129e+14        6.0\n",
      "220.642693  4.479982e+14        6.0\n",
      "227.813092  4.489247e+14        6.0\n",
      "234.983492  4.498925e+14        6.0\n",
      "242.153891  4.509015e+14        6.0\n",
      "249.324291  4.519524e+14        6.0\n",
      "256.494690  4.530439e+14        6.0\n",
      "263.665090  4.541767e+14        6.0\n"
     ]
    }
   ],
   "source": [
    "print(rss_valid_nonzeros_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only row for which non_zeros = max_nonzeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rss_valid_max_nonzeros_df = rss_valid_nonzeros_df[rss_valid_nonzeros_df[\"non_zeros\"] == max_nonzeros]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               RSS_valid  non_zeros\n",
      "156.109097  4.400374e+14        7.0\n",
      "163.279496  4.407775e+14        7.0\n",
      "170.449896  4.415667e+14        7.0\n",
      "177.620295  4.424064e+14        7.0\n",
      "184.790695  4.432967e+14        7.0\n",
      "191.961094  4.442398e+14        7.0\n",
      "199.131494  4.452307e+14        7.0\n"
     ]
    }
   ],
   "source": [
    "print(rss_valid_max_nonzeros_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUIZ QUESTIONS***\n",
    "* **What value of `l1_penalty` in our narrow range has the lowest RSS on the VALIDATION set and has sparsity *equal* to `max_nonzeros`?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS min value is: 4.40037365263e+14 for l1_penalty: 156.0\n"
     ]
    }
   ],
   "source": [
    "print(\"RSS min value is:\", rss_valid_max_nonzeros_df[\"RSS_valid\"].min(),\n",
    "      \"for l1_penalty:\",round(rss_valid_max_nonzeros_df[\"RSS_valid\"].idxmin(),0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **What features in this model have non-zero coefficients?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re-fit the model with l1_penalty giving the best RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   weight\n",
      "intercept    4.422190e+06\n",
      "bathrooms    1.061089e+04\n",
      "sqft_living  1.633803e+02\n",
      "waterfront   5.064517e+05\n",
      "view         4.196004e+04\n",
      "grade        1.162536e+05\n",
      "yr_built    -2.612235e+03\n"
     ]
    }
   ],
   "source": [
    "l1_penalty_best_rss = rss_valid_max_nonzeros_df[\"RSS_valid\"].idxmin()\n",
    "model = Lasso(alpha=l1_penalty_best_rss, normalize=True)\n",
    "model.fit(sales_train[all_features], sales_train[\"price\"])\n",
    "weight_index = all_features.copy()\n",
    "weight_index.insert(0,\"intercept\")\n",
    "features_weight = pd.DataFrame(np.append(model.intercept_,model.coef_), columns=[\"weight\"], index=weight_index)\n",
    "features_weight = features_weight[features_weight[\"weight\"]!=0]\n",
    "print(features_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
